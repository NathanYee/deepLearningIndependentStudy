{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "I recently learned that people are trying to use deep learning to solve problems from graph theory after a friend linked me the paper, [Learning Combinatorial Optimization Algorithms over Graphs](https://arxiv.org/pdf/1704.01665.pdf). I find this subject very interesting and want to explore it further. In this paper, I will try to create some simple graph representations by using vertex embeddings and a neural network. I am going to try to create what is essentially a lookup table using deep learning.\n",
    "\n",
    "This is also a great exercise for me to learn how to use PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "To create the graph, I will use a simple dictionary of nodes that contains a set of the neighbors. I will also define a next function so we can get batches of data to train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.model import fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample, randint, choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor as T\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, n_vert = 10, n_neigh = 3, bs=64):\n",
    "        \"\"\"\n",
    "        Initialize a graph object. Note that graph is not guaranteed to be\n",
    "        fully connected\n",
    "        \n",
    "        TODO:\n",
    "        Graph connectivity visualization\n",
    "        \"\"\"\n",
    "        self.bs = bs\n",
    "        self.vertices = range(n_vert)\n",
    "        self.graph = {}\n",
    "        for v in self.vertices:\n",
    "            possible = set(self.vertices).difference({v}) # remove loops\n",
    "            edges = set(sample(possible, n_neigh)) # add n neighbors to each vertex\n",
    "            self.graph[v] = edges\n",
    "            \n",
    "        for v in self.vertices:\n",
    "            for n in self.graph[v]:\n",
    "                self.graph[n].add(v)\n",
    "            \n",
    "    def pos_sample(self):\n",
    "        v = choice(self.vertices)\n",
    "        x = [v, choice(tuple(self.graph[v]))]\n",
    "        return x\n",
    "    \n",
    "    def neg_sample(self):\n",
    "        v = choice(self.vertices)\n",
    "        possible = set(self.vertices).difference(self.graph[v])\n",
    "        possible = possible.difference({v})\n",
    "        x = [v, choice(tuple(possible))]\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self, bs=64):\n",
    "        \"\"\"\n",
    "        Set the batch size\n",
    "        \n",
    "        TODO: put this in init\n",
    "        \"\"\"\n",
    "        self.bs = bs\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        Returns a batch of data for use in training. The vertices are randomly chosen\n",
    "        so it is not guaranteed that all vertices are in each training step - or that\n",
    "        every possible neighbor will be chosen. Positive and negative examples are \n",
    "        one hot encoded for simplicity\n",
    "        \"\"\"\n",
    "        n_pos = int(self.bs / 2)\n",
    "        n_neg = self.bs - n_pos\n",
    "        \n",
    "        pos = [[self.pos_sample(), [1, 0]] for _ in range(n_pos)]\n",
    "        neg = [[self.neg_sample(), [0, 1]] for _ in range(n_neg)]\n",
    "        X, Y = zip(*(pos + neg))\n",
    "        X = list(zip(*X))\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vert = 10\n",
    "n_neigh = 3\n",
    "bs = 64\n",
    "graph = Graph(n_vert=n_vert, n_neigh=n_neigh, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {1, 3, 4, 6, 7, 9},\n",
       " 1: {0, 4, 6, 7, 9},\n",
       " 2: {4, 6, 7, 8, 9},\n",
       " 3: {0, 4, 5, 6, 8},\n",
       " 4: {0, 1, 2, 3, 5, 6},\n",
       " 5: {3, 4, 6, 7, 8, 9},\n",
       " 6: {0, 1, 2, 3, 4, 5, 7, 8},\n",
       " 7: {0, 1, 2, 5, 6},\n",
       " 8: {2, 3, 5, 6, 9},\n",
       " 9: {0, 1, 2, 5, 8}}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = next(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit next(graph) # data is generated at 5,500 times per second. speed should not be an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_vert, n_neigh, nh=10, p1=0.2, p2=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        emb_dim = 5 # dimension of the vertex embedding\n",
    "        \n",
    "        self.v = nn.Embedding(n_vert, emb_dim)\n",
    "        self.v.weight.data.uniform_(-0.01, 0.01)\n",
    "#         self.v.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "        self.lin1 = nn.Linear(emb_dim*2, nh)\n",
    "        self.lin2 = nn.Linear(nh, 2)\n",
    "        self.drop1 = nn.Dropout(p1)\n",
    "        self.drop2 = nn.Dropout(p2)\n",
    "        \n",
    "    def forward(self, vertices):\n",
    "        start = vertices[0,:]\n",
    "        end = vertices[1,:]\n",
    "        x = self.drop1(torch.cat([self.v(start), self.v(end)], dim=1))\n",
    "        x = self.drop2(F.relu(self.lin1(x)))\n",
    "        x = F.softmax(self.lin2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_vert, n_neigh, nh=10, p1=0.05, p2=0.05, p3=0.05):\n",
    "        super().__init__()\n",
    "        \n",
    "        emb_dim = 5 # dimension of the vertex embedding\n",
    "        \n",
    "        self.v = nn.Embedding(n_vert, emb_dim)\n",
    "        self.v.weight.data.uniform_(-0.01, 0.01)\n",
    "        \n",
    "        self.lin1 = nn.Linear(emb_dim, nh)\n",
    "        self.lin2 = nn.Linear(nh*2, nh)\n",
    "        self.lin3 = nn.Linear(nh, 2)\n",
    "        \n",
    "        self.drop1 = nn.Dropout(p1)\n",
    "        self.drop2 = nn.Dropout(p2)\n",
    "        self.drop3 = nn.Dropout(p3)\n",
    "        \n",
    "    def forward(self, vertices):\n",
    "        start = vertices[0,:]\n",
    "        end = vertices[1,:]\n",
    "        x = F.relu(self.lin1(self.v(start)))\n",
    "        y = F.relu(self.lin1(self.v(end)))\n",
    "        x = self.drop1(torch.cat([x, y], dim=1))\n",
    "        x = self.drop2(F.relu(self.lin2(x)))\n",
    "        x = F.softmax(self.lin3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = next(graph)\n",
    "inp = Variable(torch.LongTensor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(n_vert, n_neigh).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-3\n",
    "optimizer = optim.Adam(model.parameters(), 1e-3, weight_decay=wd)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "running_corrects = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "1.00000e-03 *\n",
       " -0.0495  2.9260 -6.1389 -6.4662  5.7728\n",
       "  0.0171  0.9879 -6.2533 -5.8708 -4.8033\n",
       " -4.8220 -7.9444 -5.3494 -2.6285 -9.2742\n",
       "  1.0761 -9.0331  6.1081  8.2885  1.0739\n",
       "  9.6744 -9.0215 -3.5673  0.3676 -7.1181\n",
       "  3.4299 -5.8806  5.2835 -6.2741  2.7986\n",
       "  7.1504  7.7714 -6.4761  2.8440 -1.2669\n",
       " -1.8374 -0.7353  2.1099  6.0447 -3.1118\n",
       " -2.1981  1.5886 -6.9561  0.7860  2.0235\n",
       " -7.2763  9.1767 -4.7947  4.5776  1.3037\n",
       "[torch.cuda.FloatTensor of size 10x5 (GPU 0)]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.v.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.027\n",
      "[1,   200] loss: 0.027\n",
      "[1,   300] loss: 0.026\n",
      "[1,   400] loss: 0.026\n",
      "[1,   500] loss: 0.026\n",
      "[1,   600] loss: 0.026\n",
      "[1,   700] loss: 0.026\n",
      "[1,   800] loss: 0.026\n",
      "[1,   900] loss: 0.026\n",
      "[1,  1000] loss: 0.026\n",
      "[1,  1100] loss: 0.026\n",
      "[1,  1200] loss: 0.026\n",
      "[1,  1300] loss: 0.026\n",
      "[1,  1400] loss: 0.026\n",
      "[1,  1500] loss: 0.025\n",
      "[1,  1600] loss: 0.025\n",
      "[1,  1700] loss: 0.025\n",
      "[1,  1800] loss: 0.025\n",
      "[1,  1900] loss: 0.025\n",
      "[1,  2000] loss: 0.025\n",
      "[1,  2100] loss: 0.025\n",
      "[1,  2200] loss: 0.025\n",
      "[1,  2300] loss: 0.025\n",
      "[1,  2400] loss: 0.025\n",
      "[1,  2500] loss: 0.025\n",
      "[1,  2600] loss: 0.025\n",
      "[1,  2700] loss: 0.025\n",
      "[1,  2800] loss: 0.025\n",
      "[1,  2900] loss: 0.025\n",
      "[1,  3000] loss: 0.025\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000):\n",
    "    # get batch of data\n",
    "    X, Y = next(graph)\n",
    "    inputs = Variable(torch.LongTensor(X)).cuda()\n",
    "    labels = Variable(torch.LongTensor(Y)).cuda()\n",
    "    labels = torch.max(labels, dim=1)[1]\n",
    "\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.v.weight\n",
    "\n",
    "    running_loss += loss.data[0]\n",
    "    if i % 1000 == 999:    # print every 100 mini-batches\n",
    "        print('[%d, %5d] loss: %.3f' % (1, i + 1, running_loss / 2000))\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "-0.1180 -0.1039  0.1714 -0.0076  0.1040\n",
       "-0.0192 -0.0215 -0.0070 -0.0905  0.0272\n",
       " 0.0734  0.0600 -0.1057 -0.0196 -0.0672\n",
       "-0.0217 -0.0284  0.0179 -0.0655  0.0341\n",
       "-0.4502 -0.4815  0.3989 -0.5791  0.4789\n",
       " 0.0676  0.0581 -0.0678  0.0644 -0.0660\n",
       "-0.9655 -0.9808  0.9141 -1.0030  1.0020\n",
       " 0.3739  0.3812 -0.3967  0.4856 -0.3915\n",
       " 0.3412  0.3353 -0.3480  0.3213 -0.3450\n",
       " 0.7610  0.7670 -0.7750  0.7438 -0.7818\n",
       "[torch.cuda.FloatTensor of size 10x5 (GPU 0)]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.v.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.drop1.p = 0\n",
    "model.drop2.p = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for i in range(10):\n",
    "    # get batch of data\n",
    "    X, Y = next(graph)\n",
    "    inputs = Variable(torch.LongTensor(X)).cuda()\n",
    "    labels = Variable(torch.LongTensor(Y)).cuda()\n",
    "    labels = torch.max(labels, dim=1)[1]\n",
    "    \n",
    "    _, preds = torch.max(model(inputs), dim=1)\n",
    "    total += labels.size()[0]\n",
    "    correct += (preds == labels).sum().data[0]\n",
    "    \n",
    "model.drop1.p = .2\n",
    "model.drop2.p = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.853125"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
